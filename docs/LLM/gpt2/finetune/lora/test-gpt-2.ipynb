{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_training: dict[Literal[\"train\", \"validation\"], str] = {\n",
    "    \"train\": \"data/train_data.json\",\n",
    "    \"validation\": \"data/val_data.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"models\"\n",
    "modelID = \"openai-community/gpt2\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelID, cache_dir=cache_dir)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.padding_side = \"right\"               # Set padding side to left\n",
    "tokenizer.pad_token = tokenizer.eos_token      # Using eos_token as pad_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(modelID, device_map='auto', cache_dir=cache_dir)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # Set the pad_token_id in the model config\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\"lora.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for inference\n",
    "def generate_job_title(input_text):\n",
    "    # Prepare the input\n",
    "    input_text = f\"{input_text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)  # 1 for real tokens, 0 for padding\n",
    "\n",
    "    # Move tensors to the device (GPU if available)\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "    # Generate text with the model\n",
    "    generated_outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=input_ids.shape[-1] + 20,  # Max length is input length + 40\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,                       # Enable sampling\n",
    "        temperature=0.9,                     # Control randomness\n",
    "        top_k=50,                            # Top-K sampling\n",
    "        top_p=0.95,                          # Top-P (nucleus) sampling\n",
    "        pad_token_id=tokenizer.eos_token_id  # Padding token id\n",
    "    )\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    generated_text = tokenizer.decode(generated_outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_job_title(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"job title:\"\n",
    "input_series = pd.read_json(data_for_training['validation'])[\"text\"].apply(lambda x: x[: (len(word) + x.index(word)) ])\n",
    "input_series[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in input_series:\n",
    "    predicted_title = generate_job_title(i)\n",
    "    print(f\"\\n{predicted_title}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
