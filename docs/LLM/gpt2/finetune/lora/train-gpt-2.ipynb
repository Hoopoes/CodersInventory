{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.devgenius.io/sculpting-language-gpt-2-fine-tuning-with-lora-1caf3bfbc3c6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "from typing import Literal\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Dataset Into Training Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = \"dataset/data.json\"\n",
    "data_for_training: dict[Literal[\"train\", \"validation\"], str] = {\n",
    "    \"train\": \"data/train_data.json\",\n",
    "    \"validation\": \"data/val_data.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"models\"\n",
    "modelID = \"openai-community/gpt2\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(modelID, cache_dir=cache_dir)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.padding_side = \"right\"                # Set padding side to right\n",
    "tokenizer.pad_token = tokenizer.eos_token      # Using eos_token as pad_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(modelID, device_map='auto', cache_dir=cache_dir)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # Set the pad_token_id in the model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREEZE WEIGHTS\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,                      # Rank\n",
    "    lora_alpha=32,            # Alpha parameter for LoRA\n",
    "    lora_dropout=0.05,         # Dropout for LoRA\n",
    "    bias=\"none\",              # Choose bias (none, all, or lora)\n",
    "    task_type=TaskType.CAUSAL_LM,  # Set to Causal Language Modeling\n",
    ")\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     r=32,                      # Rank\n",
    "#     lora_alpha=16,            # Alpha parameter for LoRA\n",
    "#     lora_dropout=0.1,         # Dropout for LoRA\n",
    "#     bias=\"none\",              # Choose bias (none, all, or lora)\n",
    "#     task_type=TaskType.CAUSAL_LM,  # Set to Causal Language Modeling\n",
    "# )\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, validation, and test datasets\n",
    "dataset = load_dataset('json', data_files={\n",
    "    'train': data_for_training[\"train\"],\n",
    "    'validation': data_for_training[\"validation\"],\n",
    "    # 'test': 'data/test_data.json'\n",
    "})\n",
    "shuffled_dataset = dataset.shuffle(seed=42, keep_in_memory=True)\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your training data from train_data.json\n",
    "with open('data/train_data.json', 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "\n",
    "# Get lengths of tokenized texts\n",
    "lengths = [len(tokenizer(obj['text'])['input_ids']) for obj in train_data]\n",
    "\n",
    "def next_power_of_2(n):\n",
    "    if n < 1:\n",
    "        raise ValueError(\"Input must be a positive integer.\")\n",
    "\n",
    "    # Calculate the power of 2 using logarithm\n",
    "    power = math.ceil(math.log2(n))  # Get the smallest integer >= log2(n)\n",
    "    \n",
    "    # Return 2 raised to the calculated power\n",
    "    return 2 ** power\n",
    "\n",
    "print(\"Maximum length:\", max(lengths))\n",
    "max_length = next_power_of_2(max(lengths))\n",
    "print(\"max_length:\",max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 128\n",
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset and create labels\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_length)\n",
    "    # Create labels (shifted input for language modeling)\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_datasets = shuffled_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 8\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch,\n",
    "    gradient_accumulation_steps=batch,\n",
    "    warmup_steps=10,\n",
    "    # max_steps=500,\n",
    "    num_train_epochs=4,#20, \n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=30,\n",
    "    output_dir='outputs',\n",
    "    # auto_find_batch_size=True,\n",
    "    dataloader_drop_last=False # Don't drop the last incomplete batch (optional)\n",
    ")\n",
    "\n",
    "\n",
    "# batch = 2\n",
    "# training_args = TrainingArguments(\n",
    "#     per_device_train_batch_size=batch,\n",
    "#     gradient_accumulation_steps=batch,\n",
    "#     warmup_steps=10,\n",
    "#     # max_steps=500,\n",
    "#     num_train_epochs=12,#20, \n",
    "#     learning_rate=1e-4,\n",
    "#     logging_steps=batch*2,\n",
    "#     output_dir='outputs',\n",
    "#     auto_find_batch_size=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                        \n",
    "    args=training_args,                 \n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],  # Use validation set here\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "class CustomCallback(transformers.TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Get the training loss\n",
    "        train_loss = trainer.state.log_history[-1][\"loss\"]\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Get the evaluation loss\n",
    "        eval_loss = trainer.evaluate()[\"eval_loss\"]\n",
    "        eval_losses.append(eval_loss)\n",
    "\n",
    "# Add the custom callback to the Trainer\n",
    "trainer.add_callback(CustomCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(eval_losses) + 1), eval_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'lora.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
